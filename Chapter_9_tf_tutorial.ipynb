{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(6,name=\"x\")\n",
    "y = tf.Variable(4,name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'initializer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-436a16b7b9a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'initializer'"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global inits\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.reset_graph(seed=42)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "10\n",
      "15\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "#add variables\n",
    "\n",
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "h = x*15\n",
    "z = x*3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(w.eval())\n",
    "    print(y.eval())\n",
    "    print(z.eval())\n",
    "    print(h.eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(w.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "### Using the Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m,n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)),housing.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
       "          37.88      , -122.23      ],\n",
       "       [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
       "          37.86      , -122.22      ],\n",
       "       [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
       "          37.85      , -122.24      ],\n",
       "       ...,\n",
       "       [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
       "          39.43      , -121.22      ],\n",
       "       [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
       "          39.43      , -121.32      ],\n",
       "       [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
       "          39.37      , -121.24      ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.        ,    8.3252    ,   41.        , ...,    2.55555556,\n",
       "          37.88      , -122.23      ],\n",
       "       [   1.        ,    8.3014    ,   21.        , ...,    2.10984183,\n",
       "          37.86      , -122.22      ],\n",
       "       [   1.        ,    7.2574    ,   52.        , ...,    2.80225989,\n",
       "          37.85      , -122.24      ],\n",
       "       ...,\n",
       "       [   1.        ,    1.7       ,   17.        , ...,    2.3256351 ,\n",
       "          39.43      , -121.22      ],\n",
       "       [   1.        ,    1.8672    ,   18.        , ...,    2.12320917,\n",
       "          39.43      , -121.32      ],\n",
       "       [   1.        ,    2.3886    ,   16.        , ...,    2.61698113,\n",
       "          39.37      , -121.24      ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data_plus_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init variables for x and y\n",
    "X = tf.constant(housing_data_plus_bias,\n",
    "                dtype=tf.float32,name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1),\n",
    "                dtype=tf.float32,name='y')\n",
    "\n",
    "#traspose X\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,X)),XT),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1.           8.3252      41.        ...    2.5555556   37.88\n",
      "  -122.23     ]\n",
      " [   1.           8.3014      21.        ...    2.1098418   37.86\n",
      "  -122.22     ]\n",
      " [   1.           7.2574      52.        ...    2.80226     37.85\n",
      "  -122.24     ]\n",
      " ...\n",
      " [   1.           1.7         17.        ...    2.3256352   39.43\n",
      "  -121.22     ]\n",
      " [   1.           1.8672      18.        ...    2.1232092   39.43\n",
      "  -121.32     ]\n",
      " [   1.           2.3886      16.        ...    2.616981    39.37\n",
      "  -121.24     ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(X.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carry out computation graph\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.6955223e+01],\n",
       "       [ 4.3629766e-01],\n",
       "       [ 9.4330031e-03],\n",
       "       [-1.0657431e-01],\n",
       "       [ 6.4040172e-01],\n",
       "       [-3.9891174e-06],\n",
       "       [-3.7860912e-03],\n",
       "       [-4.2141816e-01],\n",
       "       [-4.3461639e-01]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_value\n",
    "#convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "#compare with numpy array\n",
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1,1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0133 ],\n",
       "       [-0.0004 ],\n",
       "       [-0.     ],\n",
       "       [ 0.00075],\n",
       "       [-0.00466],\n",
       "       [-0.     ],\n",
       "       [ 0.     ],\n",
       "       [-0.0001 ],\n",
       "       [-0.0001 ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare differences\n",
    "np.round((theta_value - theta_numpy),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "#compare with sklearn linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data,housing.target.reshape(-1,1))\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1,1),lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Batch Gradient Descent\n",
    "Grade Descent require the feaure vectors list. We could do this using tf, but lets compare with sklearns implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  2.34476576,  0.98214266, ..., -0.04959654,\n",
       "         1.05254828, -1.32783522],\n",
       "       [ 1.        ,  2.33223796, -0.60701891, ..., -0.09251223,\n",
       "         1.04318455, -1.32284391],\n",
       "       [ 1.        ,  1.7826994 ,  1.85618152, ..., -0.02584253,\n",
       "         1.03850269, -1.33282653],\n",
       "       ...,\n",
       "       [ 1.        , -1.14259331, -0.92485123, ..., -0.0717345 ,\n",
       "         1.77823747, -0.8237132 ],\n",
       "       [ 1.        , -1.05458292, -0.84539315, ..., -0.09122515,\n",
       "         1.77823747, -0.87362627],\n",
       "       [ 1.        , -0.78012947, -1.00430931, ..., -0.04368215,\n",
       "         1.75014627, -0.83369581]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with bias added\n",
    "scaled_housing_data_plus_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00  6.60969987e-17  5.50808322e-18  6.60969987e-17\n",
      " -1.06030602e-16 -1.10161664e-17  3.44255201e-18 -1.07958431e-15\n",
      " -8.52651283e-15]\n",
      "[ 0.38915536  0.36424355  0.5116157  ... -0.06612179 -0.06360587\n",
      "  0.01359031]\n",
      "0.11111111111111005\n",
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
    "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
    "print(scaled_housing_data_plus_bias.mean())\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually computing the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  9.16154\n",
      "Epoch 100 MSE =  6.231721\n",
      "Epoch 200 MSE =  4.316965\n",
      "Epoch 300 MSE =  3.0578644\n",
      "Epoch 400 MSE =  2.226153\n",
      "Epoch 500 MSE =  1.6748925\n",
      "Epoch 600 MSE =  1.308548\n",
      "Epoch 700 MSE =  1.0645523\n",
      "Epoch 800 MSE =  0.9017105\n",
      "Epoch 900 MSE =  0.79279906\n",
      "Epoch 1000 MSE =  0.7197754\n",
      "Epoch 1100 MSE =  0.6706612\n",
      "Epoch 1200 MSE =  0.63748986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.9029182 ],\n",
       "       [ 0.81852823],\n",
       "       [ 0.20331104],\n",
       "       [-0.31572217],\n",
       "       [ 0.339451  ],\n",
       "       [ 0.08506171],\n",
       "       [-0.03234863],\n",
       "       [-0.25900894],\n",
       "       [-0.23515938]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1300\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,\n",
    "               name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,\n",
    "               name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0,seed=42),name='theta')\n",
    "y_pred = tf.matmul(X,theta,name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error),name='mse')\n",
    "gradients = 2/m*tf.matmul(tf.transpose(X),error)\n",
    "training_op = tf.assign(theta,theta - learning_rate*gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch',epoch,\"MSE = \",mse.eval())\n",
    "        sess.run(training_op)\n",
    "    #assign\n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autodiff\n",
    "same as above except for the gradients = ...line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1500\n",
    "learning_rate = 0.03\n",
    "\n",
    "#init variables\n",
    "X = tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0,seed=42),name='theta')\n",
    "y_pred = tf.matmul(X,theta,name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error),name='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = tf.gradients(mse,[theta])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is  0 MSE =  9.16154\n",
      "Epoch is  50 MSE =  0.5856219\n",
      "Epoch is  100 MSE =  0.5555315\n",
      "Epoch is  150 MSE =  0.5460267\n",
      "Epoch is  200 MSE =  0.5396025\n",
      "Epoch is  250 MSE =  0.5351923\n",
      "Epoch is  300 MSE =  0.5321293\n",
      "Epoch is  350 MSE =  0.5299779\n",
      "Epoch is  400 MSE =  0.5284508\n",
      "Epoch is  450 MSE =  0.52735627\n",
      "Epoch is  500 MSE =  0.5265648\n",
      "Epoch is  550 MSE =  0.52598774\n",
      "Epoch is  600 MSE =  0.5255642\n",
      "Epoch is  650 MSE =  0.5252516\n",
      "Epoch is  700 MSE =  0.5250197\n",
      "Epoch is  750 MSE =  0.5248467\n",
      "Epoch is  800 MSE =  0.5247174\n",
      "Epoch is  850 MSE =  0.52462035\n",
      "Epoch is  900 MSE =  0.5245474\n",
      "Epoch is  950 MSE =  0.5244923\n",
      "Epoch is  1000 MSE =  0.5244509\n",
      "Epoch is  1050 MSE =  0.52441937\n",
      "Epoch is  1100 MSE =  0.5243956\n",
      "Epoch is  1150 MSE =  0.52437764\n",
      "Epoch is  1200 MSE =  0.524364\n",
      "Epoch is  1250 MSE =  0.5243538\n",
      "Epoch is  1300 MSE =  0.52434576\n",
      "Epoch is  1350 MSE =  0.52433985\n",
      "Epoch is  1400 MSE =  0.52433527\n",
      "Epoch is  1450 MSE =  0.5243318\n",
      "Best theta is  [[ 2.0685565 ]\n",
      " [ 0.8328103 ]\n",
      " [ 0.11938971]\n",
      " [-0.27148136]\n",
      " [ 0.31058738]\n",
      " [-0.0043123 ]\n",
      " [-0.03944827]\n",
      " [-0.892238  ]\n",
      " [-0.8632611 ]]\n"
     ]
    }
   ],
   "source": [
    "training_op = tf.assign(theta,theta - learning_rate*gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#initilizae computaiton session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 50 == 0:\n",
    "            print('Epoch is ', epoch, \"MSE = \",mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    #best theta at end of gradient descent\n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(\"Best theta is \", best_theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would find the partial derivatives ofthe following funciton with regard to a and b?\n",
    "* you would need to calculate the derivatives at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(a,b):\n",
    "    z = 0\n",
    "    for i in range(100):\n",
    "        z = a*np.cos(z+i) + z*np.sin(b-i)\n",
    "    return(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21253923284754916"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func(0.2,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "a = tf.Variable(0.2,name=\"a\")\n",
    "b = tf.Variable(0.3,name=\"b\")\n",
    "z = tf.Variable(0.0,name=\"z0\")\n",
    "\n",
    "for i in range(100):\n",
    "    z = a*tf.cos(z+i) + z*tf.sin(b-i)\n",
    "    \n",
    "#compute the gradients of z at a,b\n",
    "grads = tf.gradients(z,[a,b])\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.3\n",
      "[-1.1388494, 0.19671397]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(a.eval())\n",
    "    print(b.eval())\n",
    "    print(sess.run(grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took a few seconds to run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a $GradientDescentOptimizer$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0,seed=42),name='theta')\n",
    "y_pred = tf.matmul(X,theta,name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error),name='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate= learning_rate)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 MSE =  9.16154\n",
      "Epoch  100 MSE =  0.71450037\n",
      "Epoch  200 MSE =  0.56670487\n",
      "Epoch  300 MSE =  0.55557173\n",
      "Epoch  400 MSE =  0.5488112\n",
      "Epoch  500 MSE =  0.54363626\n",
      "Epoch  600 MSE =  0.53962904\n",
      "Epoch  700 MSE =  0.5365092\n",
      "Epoch  800 MSE =  0.5340677\n",
      "Epoch  900 MSE =  0.5321473\n",
      "Best theta is  [[ 2.0685525 ]\n",
      " [ 0.8874027 ]\n",
      " [ 0.14401656]\n",
      " [-0.34770876]\n",
      " [ 0.36178365]\n",
      " [ 0.00393811]\n",
      " [-0.04269557]\n",
      " [-0.66145283]\n",
      " [-0.63752776]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch \",epoch, \"MSE = \",mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(\"Best theta is \",best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0,seed=42),name='theta')\n",
    "y_pred = tf.matmul(X,theta,name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error),name='mse')\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init training op and session and variables\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta\n",
      "[[ 2.068558  ]\n",
      " [ 0.8296283 ]\n",
      " [ 0.11875331]\n",
      " [-0.26554415]\n",
      " [ 0.3057106 ]\n",
      " [-0.00450251]\n",
      " [-0.0393266 ]\n",
      " [-0.8998653 ]\n",
      " [-0.8705215 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(\"Best theta\") \n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try feeding training data into training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder nodes\n",
    "* you can't actuall see the placehold nodes when you evaluate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "A = tf.placeholder(tf.float32,shape=(None,3))\n",
    "B = A + 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1,2,3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4,5,6],[7,8,9]]})\n",
    "print(B_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n+1),name='X')\n",
    "y = tf.placeholder(tf.float32,shape=(None,1),name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0,seed=42),name='theta')\n",
    "y_pred = tf.matmul(X,theta,name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error),name='mse')\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch(epoch,batch_index,batch_size):\n",
    "    np.random.seed(epoch*n_batches + batch_index)\n",
    "    indices = np.random.randint(m,size = batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]\n",
    "    y_batch = housing.target.reshape(-1,1)[indices]\n",
    "    return(X_batch,y_batch)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch,y_batch = fetch_batch(epoch,batch_index,batch_size)\n",
    "            sess.run(training_op,feed_dict={X:X_batch,y:y_batch})\n",
    "        \n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0703337 ],\n",
       "       [ 0.8637145 ],\n",
       "       [ 0.12255149],\n",
       "       [-0.31211883],\n",
       "       [ 0.38510382],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.8030471 ]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and restoring a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  9.16154\n",
      "Epoch 100 MSE =  0.71450037\n",
      "Epoch 200 MSE =  0.56670487\n",
      "Epoch 300 MSE =  0.55557173\n",
      "Epoch 400 MSE =  0.5488112\n",
      "Epoch 500 MSE =  0.54363626\n",
      "Epoch 600 MSE =  0.53962904\n",
      "Epoch 700 MSE =  0.5365092\n",
      "Epoch 800 MSE =  0.5340677\n",
      "Epoch 900 MSE =  0.5321473\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0,seed=42),name='theta')\n",
    "y_pred = tf.matmul(X,theta,name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error),name='mse')\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\",epoch, \"MSE = \",mse.eval())\n",
    "            save_path = saver.save(sess,'my_model.ckpt')\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess,\"my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685525 ],\n",
       "       [ 0.8874027 ],\n",
       "       [ 0.14401656],\n",
       "       [-0.34770876],\n",
       "       [ 0.36178365],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269557],\n",
       "       [-0.66145283],\n",
       "       [-0.63752776]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'my_model_final.ckpt')\n",
    "    best_theta_restores = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to have a svaer that loads and rstores theta with a different name such as weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver({\"weights\":theta})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.saver.Saver at 0x1c46db5c88>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by default the saver alos saves the grpah structure itslef in a second extension file .meta. You can use the funtion tf.train.import_meta_graph() to restore the graph structure. This function loads the graph into the default graph and returns a save tha can then be sued to restore the graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "#make sure to start with an empty graph\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"my_model_final.ckpt.meta\") # this load the graph structure\n",
    "\n",
    "theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685525 ],\n",
       "       [ 0.8874027 ],\n",
       "       [ 0.14401656],\n",
       "       [-0.34770876],\n",
       "       [ 0.36178365],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269557],\n",
       "       [-0.66145283],\n",
       "       [-0.63752776]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta_restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta,best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, we have imported a pretraing model without having to have the correspoding pythong code to build the gprahy. This is very handy when you keep tweaking and saving your model. You can load a previously saved model without having to search for the ersion of the code that built it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "def fetch_batch(epoch,batch_index,batch_size):\n",
    "    np.random.seed(epoch*n_batches + batch_index)\n",
    "    indices = np.random.randint(m,size = batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]\n",
    "    y_batch = housing.target.reshape(-1,1)[indices]\n",
    "    return(X_batch,y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                                                        # not shown in the book\n",
    "    sess.run(init)                                                                # not shown\n",
    "\n",
    "    for epoch in range(n_epochs):                                                 # not shown\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0703337 ],\n",
       "       [ 0.8637145 ],\n",
       "       [ 0.12255149],\n",
       "       [-0.31211883],\n",
       "       [ 0.38510382],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.8030471 ]], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None, n+1),name= \"X\")\n",
    "y = tf.placeholder(tf.float32,shape=(None, 1),name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0,seed= 42),name='theta')\n",
    "y_pred = tf.matmul(X,theta,name='predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combing error and mse just calling it loss\n",
    "with tf.name_scope('loss') as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error),name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar(\"MSE\",mse)\n",
    "file_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.0703337 ]\n",
      " [ 0.8637145 ]\n",
      " [ 0.12255149]\n",
      " [-0.31211883]\n",
      " [ 0.38510382]\n",
      " [ 0.00434168]\n",
      " [-0.01232954]\n",
      " [-0.83376896]\n",
      " [-0.8030471 ]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m/batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch,y_batch = fetch_batch(epoch,batch_index,batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X:X_batch,y:y_batch})\n",
    "                step = epoch*n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str,step)\n",
    "            sess.run(training_op,feed_dict={X:X_batch,y:y_batch})\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "file_writer.flush()\n",
    "file_writer.close()\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loss/sub'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error.op.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loss/mse'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse.op.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modularity\n",
    "* a really ugly flat code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((n_features,1)),name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features,2)),name=\"weights2\")\n",
    "b1 = tf.Variable(0.0,name='bias1')\n",
    "b2 = tf.Variable(0.0,name='bias2')\n",
    "\n",
    "z1 = tf.add(tf.matmul(X,w1),b1,name='z1')\n",
    "z2 = tf.add(tf.matmul(X,w2),b2,name='z2')\n",
    "\n",
    "relu1 = tf.maximum(z1,0.,name='relu1')\n",
    "relu2 = tf.maximum(z1,0.,name='relu2')\n",
    "\n",
    "output = tf.add(relu1,relu2,name='output')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function to build relus after input nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    #get shape if inputs\n",
    "    w_shape = (int(X.get_shape()[1]),1)\n",
    "    #assign weights randmoaly\n",
    "    w = tf.Variable(tf.random_normal(w_shape),name='weights')\n",
    "    #bias starting at zerp\n",
    "    b = tf.Variable(0.0,name='bias')\n",
    "    #wx+b on the node\n",
    "    z = tf.add(tf.matmul(X,w),b,name='z')\n",
    "    return(tf.maximum(z,0.,name='relu'))\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "relus =[relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus,name = 'ouput')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to file\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu1\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better using name scopes!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "def relu(X):\n",
    "    #init session\n",
    "    with tf.name_scope('relu'):\n",
    "        w_shape = (int(X.get_shape()[1]),1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape),name='weights')\n",
    "        b = tf.Variable(0.0,name='bias')\n",
    "        z = tf.add(tf.matmul(X,w),b,name='z')\n",
    "        return(tf.maximum(z,0.,name='max'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu2\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharing Variables\n",
    "sharing a threshold the classic way, by defining it ouside the relu() funciotnthen passing it as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X,threshold):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]),1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape),name='weights')\n",
    "        b = tf.Variable(0.0,name='bias')\n",
    "        z = tf.add(tf.matmul(X,w),b,name='z')\n",
    "        return(tf.maximum(z,threshold,name='max'))\n",
    "    \n",
    "threshold = tf.Variable(0.0,name='threshold')\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "relus = [relu(X,threshold) for i in range(5)]\n",
    "output = tf.add_n(relus,name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu,\"threshold\"):\n",
    "            relu.threshold = tf.Variable(0.0,name='threshold')\n",
    "        w_shape = int(X.get_shape()[1]),1\n",
    "        w = tf.Variable(tf.random_normal(w_shape),name='weights')\n",
    "        b = tf.Variable(0.0,name='bias')\n",
    "        z = tf.add(tf.matmul(X,w),b,name='z')\n",
    "        return(tf.maximum(z,relu.threshold,name='max'))\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus,name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring threshold variable first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope('relu'):\n",
    "    threshold = tf.get_variable(\"threshold\",shape=(),initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\",reuse=True):\n",
    "    threshold = tf.get_variable('threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\"):\n",
    "        threshold = tf.get_variable(\"threshold\",shape=(),initializer=tf.constant_initializer(0.0))\n",
    "        w_shape = (int(X.get_shape()[1]),1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape),name='weights')\n",
    "        b = tf.Variable(0.0,name='bias')\n",
    "        z = tf. add(tf.matmul(X,w),b,name=\"z\")\n",
    "        return(tf.maximum(z,threshold,name='max'))\n",
    "    \n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "with tf.variable_scope('',default_name=\"\") as scope:\n",
    "    first_relu = relu(X) #create the shared vairable\n",
    "    scope.reuse_variables()\n",
    "    relus = [first_relu] + [relu(X) for i in range(4)]\n",
    "    \n",
    "output = tf.add_n(relus,name='output')\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu8\",tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "    b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "    return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(6):\n",
    "    with tf.variable_scope(\"relu\",reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "\n",
    "output = tf.add_n(relus,name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu9\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0: my_scope/x\n",
      "x1: my_scope/x_1\n",
      "x2: my_scope/x_2\n",
      "x3: my_scope/x\n",
      "x4: my_scope_1/x\n",
      "x5: my_scope/x\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope('my_scope'):\n",
    "    x0 = tf.get_variable(\"x\",shape=(),initializer=tf.constant_initializer(0.))\n",
    "    x1 = tf.Variable(0.,name='x')\n",
    "    x2 = tf.Variable(0.,name='x')\n",
    "    \n",
    "with tf.variable_scope(\"my_scope\",reuse=True):\n",
    "    x3 = tf.get_variable(\"x\")\n",
    "    x4 = tf.Variable(0.,name='x')\n",
    "    \n",
    "with tf.variable_scope(\"\",default_name=\"\",reuse=True):\n",
    "    x5 = tf.get_variable(\"my_scope/x\")\n",
    "    \n",
    "print('x0:', x0.op.name)\n",
    "print('x1:', x1.op.name)\n",
    "print('x2:', x2.op.name)\n",
    "print('x3:', x3.op.name)\n",
    "print('x4:', x4.op.name)\n",
    "print('x5:', x5.op.name)\n",
    "print(x0 is x3)\n",
    "print(x3 is x5)\n",
    "print(x0 is x3 and x3 is x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first variable_scope() block first creates the shared variable x0, named my_scope/x. For all operations other than shared variables (including non-shared variables), the variable scope acts like a regular name scope, which is why the two variables x1 and x2 have a name with a prefix my_scope/. Note however that TensorFlow makes their names unique by adding an index: my_scope/x_1 and my_scope/x_2.\n",
    "\n",
    "The second variable_scope() block reuses the shared variables in scope my_scope, which is why x0 is x3. Once again, for all operations other than shared variables it acts as a named scope, and since it's a separate block from the first one, the name of the scope is made unique by TensorFlow (my_scope_1) and thus the variable x4 is named my_scope_1/x.\n",
    "\n",
    "The third block shows another way to get a handle on the shared variable my_scope/x by creating a variable_scope() at the root scope (whose name is an empty string), then calling get_variable() with the full name of the shared variable (i.e. \"my_scope/x\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Do' b'you' b'want' b'some' b'cafe?']\n",
      "b'akgkjggjag'\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "text = np.array(\"Do you want some cafe?\".split())\n",
    "text1 = np.array(\"akgkjggjag\")\n",
    "text_tensor = tf.constant(text)\n",
    "text_tensor_1 = tf.constant(text1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(text_tensor.eval())\n",
    "    print(text_tensor_1.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aside, autodiff\n",
    "\n",
    "## Introduction\n",
    "Suppose we want to compute the gradients of the funciton $f(x,y) = x^2 y + y + 2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    return x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute partials analytically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\frac{\\partial f}{\\partial x} = 2xy \\\\\n",
    "\\frac{\\partial f}{\\partial y} = x^2 + 1\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x,y):\n",
    "    return 2*x*y, x*x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for example:\n",
    "$\n",
    "\\frac{\\partial f}{\\partial x} (3,4) = 24\\\\\n",
    "\\frac{\\partial f}{\\partial y} (3,4) = 10\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 10)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the second partial and mixed dereivates:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial x} = \\frac{\\partial (2xy)}{\\partial x} = 2y \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial (2xy)}{\\partial y} = 2x \\\\\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial (x^2 + 1)}{\\partial x} = 2x \\\\\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial y} = \\frac{\\partial (x^2 + 1)}{\\partial y} = 0\n",
    "\\\\\\end{align}\n",
    "\n",
    "at (3,4) the hessian is 8,6,6,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2f(x, y):\n",
    "    return [2*y, 2*x], [2*x, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8, 6], [6, 0])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2f(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! But this requires some mathematical work and too many computations for a deep neural entwork. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical differentiation\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial x} = \\lim_{\\epsilon \\to 0} \\frac{f(x+\\epsilon,y) - f(x,y)}{\\epsilon}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(func,vars_list,eps=0.000001):\n",
    "    #store partial derivaties\n",
    "    partial_derivatives = []\n",
    "    #apply base func to arguemtns\n",
    "    base_func_eval = func(*vars_list)\n",
    "    for idx in range(len(vars_list)):\n",
    "        #pull values\n",
    "        tweaked_vars = vars_list[:]\n",
    "        #advance a bit\n",
    "        tweaked_vars[idx] +=eps\n",
    "        #apply func\n",
    "        tweaked_func_eval = func(*tweaked_vars)\n",
    "        #d dx\n",
    "        derivative = (tweaked_func_eval - base_func_eval) / eps\n",
    "        #store\n",
    "        partial_derivatives.append(derivative)\n",
    "    return(partial_derivatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x, y):\n",
    "    return gradients(f, [x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24.000004003710274, 9.99999999606871]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works! Now lets try computing the Hessians!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.000004003710274, 9.99999999606871)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dfdx(x,y):\n",
    "    return(gradients(f,[x,y])[0])\n",
    "\n",
    "def dfdy(x,y):\n",
    "    return(gradients(f,[x,y])[1])\n",
    "\n",
    "dfdx(3.,4.),dfdy(3.,4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can apply the gradients function for all orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2f(x,y):\n",
    "    return([gradients(dfdx,[3.,4.]),gradients(dfdy,[3.,4.])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8.000711204658728, 6.004086117172847],\n",
       " [6.004086117172847, 0.014210854715202004]]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2f(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So everything wokrd well, but the result is only approximate and computing the gradients of a function with regards to $n$ variables requires calling that function n times. In deep neural nets, there are often thousands of parameters to tweak using gradient descent (which requires copmuting the gradients of the loss funciton with regard to each of these parameters). So this approach would be much too slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a to computation graph\n",
    "Rathern than this numerical approach, lets implement some symbolic autodiff techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create class for constant variables\n",
    "\n",
    "class Const(object):\n",
    "    def __init__(self,value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return(self.value)\n",
    "    def __str__(self):\n",
    "        return(str(self.value))\n",
    "    \n",
    "class Var(object):\n",
    "    def __init__(self,name,init_value=0):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "    def evaluate(self):\n",
    "        return(self.value)\n",
    "    def __str__(self):\n",
    "        return(self.name)\n",
    "    \n",
    "class BinaryOperator(object):\n",
    "    def __init__(self,a,b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "    \n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return(self.a.evaluate() + self.b.evaluate())\n",
    "    def __str__(self):\n",
    "        return(\"{} + {}\".format(self.a,self.b))\n",
    "    \n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return(self.a.evaluate()*self.b.evaluate())\n",
    "    def __str__(self):\n",
    "        return(\"({})+({})\".format(self.a(),self.b()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a copmutation grpah to represent the function $f$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = Var(\"x\")\n",
    "y = Var(\"y\")\n",
    "f = Add(Mul(Mul(x,x),y),Add(y,Const(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value = 3\n",
    "y.value = 4\n",
    "f.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOOOHOO! Starting to get a hang of classes now and it found the perfect answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the gradients\n",
    "The autodiff method we present below are all base on the chain rule.\n",
    "\n",
    "Suppose we have two functions $u$ and $v$ and we apply them sequentially to some nput $x$ and we the result $z$. So we have $z=v(u(x))$ which we can rewrite as $z=v(s)$ and $s=u(x)$. Now we apply the chain rule to get the partial derivative of the output $z$ with regards to the input $x$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial s}{\\partial x} \\cdot \\frac{\\partial x}{\\partial s}\n",
    "\\end{align}\n",
    "\n",
    "Now if $z$ is the ouput of a sequence of functions which have intermediate outputs $s_1,s_2..., s_n$ the chain rule still applies\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial s_1}{\\partial x} \\cdot \\frac{\\partial s_2}{\\partial s_1} \\cdot \\frac{\\partial s_3}{\\partial s_2} ...\\frac{\\partial s_{n-1}}{\\partial s_{n-1}} \\cdot \\frac{\\partial s_n}{\\partial s_{n-1}} \\cdot \\frac{\\partial z}{\\partial s_n} \n",
    "\\end{align}\n",
    "\n",
    "In forward mode autodii, the algorith computes these terms \"forward: (i.e inthe smae order as the computations regquired to compute the output $z$), that is from the left to the right so;\n",
    "\n",
    "\\begin{align}\n",
    "First: \\\\\n",
    "\\frac{\\partial s_1}{\\partial x} \\\\\n",
    "Second: \\\\\n",
    "\\frac{\\partial s_2}{\\partial s_1} \\\\\n",
    "\\end{align}\n",
    "\n",
    "And so on.\n",
    "\n",
    "In reverse mode autodiff, the algorithm computes these terms backwards from right to left. First $\\frac{\\partial z}{\\partial s_n}$, then $\\frac{\\partial s_n}{\\partial s_{n-1}}$ and so on. \n",
    "\n",
    "For exmplae, suppose you wanted to compute the deriavate of the function $z(x) = sin(x^2)$ at $x=3$ using forward mode autodiff. The algorithm would first compute the partial derivative:\n",
    "\\begin{align}\n",
    "\\frac{\\partial s_1}{\\partial x} = \\frac{\\partial x^2}{\\partial x} = 2x = 6\n",
    "\\end{align}\n",
    "\n",
    "Next it owuld compute:\n",
    "\\begin{align}\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial s}{\\partial x} \\cdot \\frac{\\partial z}{\\partial s_1} = 6 \\cdot \\frac{\\partial sin(s_1)}{\\partial s_1} = 6 \\cdot cos(s_1) = 6 \\cdot(3^2) \\approx -5.46 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Lets verify thsi results using the gradients function defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5.466789901376057]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sin\n",
    "\n",
    "def z(x):\n",
    "    return(sin(x**2))\n",
    "\n",
    "gradients(z,[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Now lets the same thing using reverse mode autodiff. This time the algorithm would start from the right hand side so it would first compute\n",
    "\\begin{align}\n",
    "\\frac{\\partial z}{\\partial s_1} = \\frac{\\partial sin(s_1)}{\\partial s_1} = cos(s_1) = cos(3^2) \\approx -0.91 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Next it would copmute:\n",
    "\\begin{align}\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial s_1}{\\partial x} \\cdot \\frac{\\partial z}{\\partial s_1} \\approx \\frac{\\partial s_1}{\\partial x} \\cdot -0.91 = \\frac{\\partial x^2}{\\partial x} \\cdot -0.91 = 2x \\cdot -0.91 = 6 \\cdot -0.1 = -5.46\n",
    "\\end{align}\n",
    "\n",
    "In review, both approaches give the same result (except for rounding errors) and with a single input and out put they involve the same number of computations. But when there are several inputs or outputs, they can have very different performance. Indeed, if there are many inputs, the right most terms  will be need to computed  the partial derivatives with regards to each input, so it is a good idea to compute these right most terms first. THIS MEANS USING REVERSE MODE AUTODIFF!!! This way the right most terms can be computed just once and used to copmute all the partial derivatives. Conversely, if there are many outputs, forward mode is generally preferable because the left most terms can be computes just once to compute the partial derivatives. Converse;y, if there are many outputs, forward mode is generally preferable because the left most terms can be computed just once to compute the partial derivatives of the different outputs. In Deep Learning, there are typically thousands of model parameters, meaning there are lots of onputs, but few outputs. In fact, there is generally just one output during the training: THE LOSS! This is why reverse mode auto diff is used in TensorFlow and all major Deep Learning Libraries\n",
    "\n",
    "There's one additional complexity in reverse mode autodiff, the value of $s_i$ is generally required when computing $$\\frac{\\partial}{\\partial s_i} (s_i + 1)$$ and computing $s_i$ requires  first computing $s_{i-1}$, which requires computing $s_{i-2}$ and so on. So bascailly, a first pass forward through the network is required to compute $s_1,s_2,s_3...s_{n-1}$ and then the algorithm can compute the partial deriviatives from right to left. Storing all the intermediate values $s_i$ in RAM is sometimes a problem, especially when handling images and when using GPUS which often have limited RAM. To limit this problem, one can reuce the number of layers in the neural network or configure Tensorflow to make it swap these values from GPU RAM to CPU RAM. Another approach is to only cahcer every other intermediate value, so $s_1, s_3, s_5...s_{n-4},s_{n-2}$. This means that when the algorithm computes the partial deriavtives, if an intermediate value $s_i$ is missing, it will ned to recompute based on the previous intermediate value $s_{i-1}$. This trades off CPU for RAM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward mode autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "Const.gradient = lambda self, var: Const(0)\n",
    "Var.gradient = lambda self, var: Const(1) if self is var else Const(0)\n",
    "Add.gradient = lambda self, var: Add(self.a.gradient(var), self.b.gradient(var))\n",
    "Mul.gradient = lambda self, var: Add(Mul(self.a, self.b.gradient(var)), Mul(self.a.gradient(var), self.b))\n",
    "\n",
    "x = Var(name=\"x\", init_value=3.)\n",
    "y = Var(name=\"y\", init_value=4.)\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xy + y + 2\n",
    "\n",
    "dfdx = f.gradient(x)  # 2xy\n",
    "dfdy = f.gradient(y)  # x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.0, 10.0)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfdx.evaluate(), dfdy.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ouput of the gradient() method is fully symbolic, we are not limited to the first order deriviatives, we can also compute second order derivatives and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2fdxdx = dfdx.gradient(x) #2y\n",
    "d2fdxdy = dfdx.gradient(y) #2x\n",
    "d2fdydx = dfdy.gradient(x) #2x\n",
    "d2fdydy = dfdy.gradient(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8.0, 6.0], [6.0, 0.0]]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[d2fdxdx.evaluate(), d2fdxdy.evaluate()],\n",
    " [d2fdydx.evaluate(), d2fdydy.evaluate()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the result is exact, up to machine precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward mode using autodiff using dual numbers\n",
    "A nice way to apply forward mode autodiff is to use dual numbers. In short, a dual number $z$ has the form $z = a + b \\epsilon$, where $a$ and $b$ are real numbers, and $\\epsilon$ is an infinitsemal number, positive but smaller than all real numbers such that $\\epsilon^2 = 0$. It can be shown that $f(x+\\epsilon) = f(x) + \\frac{\\partial f}{\\partial x} \\epsilon$, so simply computing $f(x+ \\epsilon)$ we get both the value of $f(x)$ wand the partial driative of $f$ w.r.t $x$.\n",
    "\n",
    "Dual number have their own arithmeti rules:\n",
    "\n",
    "Addition: \n",
    "$(a_1 + b_1 \\epsilon) + (a_2 + b_2 \\epsilon) = (a_1 + a_2) + (b_1 + b_2)\\epsilon$\n",
    "\n",
    "Subtraction:\n",
    "$(a_1 + b_1 \\epsilon) - (a_2 + b_2 \\epsilon) = (a_1 - a_2) + (b_1 - b_2)\\epsilon$\n",
    "\n",
    "Multiplacation:\n",
    "$(a_1 + b_1 \\epsilon) \\times (a_2 + b_2 \\epsilon) = (a_1 a_2) + (a_1 b_2 + a_2 b_1)\\epsilon + (b_1 b_2) \\epsilon^2 = (a_1 a_2) + (a_1 b_2 + a_2 b_1)\\epsilon$\n",
    "\n",
    "Division:\n",
    "$\\frac{a_1 + b_1 \\epsilon}{a_2 + b_2 \\epsilon} = \\frac{a_1}{a_2} + \\frac{a_1 b_2 - b_1 a_2}{{a_2}^2} \\epsilon$\n",
    "\n",
    "Power:\n",
    "$(a+ b \\epsilon)^{n} = a^n + (na^{n-1}b)\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualNumber(object):\n",
    "    def __init__(self, value=0.0, eps=0.0):\n",
    "        self.value = value\n",
    "        self.eps = eps\n",
    "    def __add__(self, b):\n",
    "        return DualNumber(self.value + self.to_dual(b).value,\n",
    "                          self.eps + self.to_dual(b).eps)\n",
    "    def __radd__(self, a):\n",
    "        return self.to_dual(a).__add__(self)\n",
    "    def __mul__(self, b):\n",
    "        return DualNumber(self.value * self.to_dual(b).value,\n",
    "                          self.eps * self.to_dual(b).value + self.value * self.to_dual(b).eps)\n",
    "    def __rmul__(self, a):\n",
    "        return self.to_dual(a).__mul__(self)\n",
    "    def __str__(self):\n",
    "        if self.eps:\n",
    "            return \"{:.1f} + {:.1f}\".format(self.value, self.eps)\n",
    "        else:\n",
    "            return \"{:.1f}\".format(self.value)\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    @classmethod\n",
    "    def to_dual(cls, n):\n",
    "        if hasattr(n, \"value\"):\n",
    "            return n\n",
    "        else:\n",
    "            return cls(n)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3 + (3+ 4 \\epsilon) = 6 + 4 \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0 + 4.0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 + DualNumber(3, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "(3 + 4 \\epsilon) \\times (5 + 7 \\epsilon)\\\\\n",
    "3\\times5 + 3\\times7 \\epsilon + 4\\epsilon \\times 5 + 4\\epsilon \\times 7 \\epsilon \\\\\n",
    "15 + 21 \\epsilon  + 20 \\epsilon  + 28 \\epsilon \\epsilon \\\\\n",
    "15 + 41 \\epsilon\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0 + 41.0"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DualNumber(3, 4) * DualNumber(5, 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value = DualNumber(3.0)\n",
    "y.value = DualNumber(4.0)\n",
    "\n",
    "f.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything works! Now lets see if the dual numbers work with out toy computation framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.value = DualNumber(3.0,1.0)\n",
    "y.value = DualNumber(4.0)\n",
    "\n",
    "dfdx = f.evaluate().eps\n",
    "\n",
    "x.value = DualNumber(3.0)\n",
    "y.value = DualNumber(4.0,1.0)\n",
    "\n",
    "dfdy = f.evaluate().eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.0, 10.0)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfdx, dfdy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse mode autodiff\n",
    "\n",
    "Lets rewrite our toy framework to add reverse mode autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Const(object):\n",
    "    def __init__(self,value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return(self.value)\n",
    "    def backpropagate(self,gradient):\n",
    "        pass\n",
    "    def __str__(self):\n",
    "        return(str(self.value))\n",
    "    \n",
    "class Var(object):\n",
    "    def __init__(self,name,init_value = 0):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "        self.gradient = 0\n",
    "    def evaluate(self):\n",
    "        return(self.value)\n",
    "    def backpropagate(self,gradient):\n",
    "        self.gradient += gradient\n",
    "    def __str__(self):\n",
    "        return(self.name)\n",
    "    \n",
    "class BinaryOperator(object):\n",
    "    def __init__(self,a,b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        \n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() + self.b.evaluate()\n",
    "        return(self.value)\n",
    "    def backpropagate(self,gradient):\n",
    "        self.a.backpropagate(gradient)\n",
    "        self.b.backpropagate(gradient)\n",
    "    def __str__(self):\n",
    "        return(\"{} + {}\".format(self.a,self.b))\n",
    "    \n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate()*self.b.evaluate()\n",
    "        return(self.value)\n",
    "    def backpropagate(self,gradient):\n",
    "        self.a.backpropagate(gradient*self.b.value)\n",
    "        self.b.backpropagate(gradient*self.a.value)\n",
    "    def __str__(self):\n",
    "        return(\"{} * {}\".format(self.a,self.b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Var(\"x\",init_value=3)\n",
    "y = Var(\"y\", init_value=4)\n",
    "f = Add(Mul(Mul(x,x),y),Add(y,Const(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = f.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backpropagate(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x * x * y + y + 2\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation the outputs are just numbers not symbolic expressions, so we are limited to first order deriviatvies. Howevver, we could have mode the backpragate() methods return smbolix expression rathern than values (e.g. return Add(2,3) rater than5). This would make it possible to compute second order gradients and beyone. This is what tf does!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse mode autodiff using Tensforflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.0, [24.0, 10.0])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.Variable(3.,name=\"x\")\n",
    "y = tf.Variable(4.,name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "jacobians = tf.gradients(f,[x,y])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    f_val,jacobians_val = sess.run([f,jacobians])\n",
    "    \n",
    "f_val,jacobians_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since everything is symbollic we can compute the second order derivatives and beyong. However, when we compute the derivative of a tensor with regards to a variable that it does not depend on, instead of returning a 0.0, the gradients() function returns None! which cannot be evaluted by sess.run(). So beward of None values!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8.0, 6.0], [6.0, 0.0])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessians_x = tf.gradients(jacobians[0],[x,y])\n",
    "hessians_y = tf.gradients(jacobians[1],[x,y])\n",
    "\n",
    "def replace_none_with_zero(tensors):\n",
    "    return([tensor if tensor is not None else tf.constant(0.) for tensor in tensors])\n",
    "\n",
    "hessians_x = replace_none_with_zero(hessians_x)\n",
    "hessians_y = replace_none_with_zero(hessians_y)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    hessians_x_val, hessians_y_val = sess.run([hessians_x,hessians_y])\n",
    "    \n",
    "hessians_x_val,hessians_y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TADA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63789964\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "x = tf.Variable(tf.random_uniform(shape=(),minval=0.0,maxval=1.0))\n",
    "x_new_val = tf.placeholder(shape=(),dtype=tf.float32)\n",
    "x_assign = tf.assign(x,x_new_val)\n",
    "\n",
    "with tf.Session():\n",
    "    x.initializer.run()\n",
    "    print(x.eval())\n",
    "    #reassign\n",
    "    x_assign.eval(feed_dict={x_new_val:5.0})\n",
    "    print(x.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12 log regression with mini batch gradient descent using tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "m = 1000\n",
    "X_moons,y_moons = make_moons(m,noise =0.1, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1], 'go', label=\"Positive\")\n",
    "plt.plot(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absorb bias $x_0 = 1$ into the  inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons_with_bias = np.c_[np.ones((m,1)),X_moons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.97416561, -0.10979094],\n",
       "       [ 1.        ,  2.05086911,  0.09086988],\n",
       "       [ 1.        , -0.1261885 ,  0.26773291],\n",
       "       ...,\n",
       "       [ 1.        ,  2.06779572,  0.18091295],\n",
       "       [ 1.        , -0.7862558 ,  0.64236981],\n",
       "       [ 1.        ,  1.16812572, -0.43787438]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_moons_with_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape y train to get into a 2d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_moons_column_vector = y_moons.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split\n",
    "test_ratio = 0.2\n",
    "test_size = int(m * test_ratio)\n",
    "X_train = X_moons_with_bias[:-test_size]\n",
    "X_test = X_moons_with_bias[-test_size:]\n",
    "y_train = y_moons_column_vector[:-test_size]\n",
    "y_test = y_moons_column_vector[-test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now lets creat a small function to generate training batches. In this implementation we randomly pick trainig smaples. Meaning that a single batch may contain the same instance multiple times, and therefore a single epoch may not cover all the training instances. However, this is generally not a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X_train,y_train,batch_size):\n",
    "    rnd_indices = np.random.randint(0,len(X_train),batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return(X_batch,y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.18235972,  0.86980148],\n",
       "       [ 1.        ,  1.94277122,  0.41861651],\n",
       "       [ 1.        ,  1.65106036, -0.27800204],\n",
       "       [ 1.        ,  0.90758167, -0.00555286],\n",
       "       [ 1.        , -0.73239699,  0.95218093]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = random_batch(X_train, y_train, 5)\n",
    "X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letw slowly build up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "n_inputs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a logistic regression model: \n",
    "\n",
    "Recall:\n",
    "$\\hat{p} = h_\\theta (x) = \\sigma (\\theta^T x)$\n",
    "\n",
    "The $\\theta$ parameter vector, containing the bias term $\\theta_0$ and the weights thetas through theta n contains the bias term $x_0 = 1$ as well as the input features.\n",
    "\n",
    "Since we want to be able to make predicitons for multiple instnaces at a tme, we will an input matrix $\\bf{X}$ rather than a sinlge input vector for each instance. It then becomes:\n",
    "\n",
    "$\\hat{p} = \\sigma (\\bf{(X\\theta)}$\n",
    "\n",
    "Now we can build the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,shape = (None, n_inputs + 1),name=\"X\")\n",
    "y = tf.placeholder(tf.float32,shape=(None,1),name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n_inputs + 1,1],-1.0,1.0,seed=42),name='theta')\n",
    "logits = tf.matmul(X,theta,name='logits')\n",
    "y_proba = 1 / (1+ tf.exp(-logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = tf.sigmoid(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\boldsymbol{\\theta}) = -\\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{\\left[ y^{(i)} \\log\\left(\\hat{p}^{(i)}\\right) + (1 - y^{(i)}) \\log\\left(1 - \\hat{p}^{(i)}\\right)\\right]}$\n",
    "\n",
    "Implement ouselves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7\n",
    "loss = -tf.reduce_mean(y*tf.log(y_proba+epsilon)  + (1-y)*tf.log(1-y_proba + epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But could also just use the log loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.log_loss(y,y_proba) #uses 1e-7 by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the rest of the computation grpah\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the rest of the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.8182378\n",
      "Epoch: 100 \tLoss: 0.3180591\n",
      "Epoch: 200 \tLoss: 0.28040478\n",
      "Epoch: 300 \tLoss: 0.26421708\n",
      "Epoch: 400 \tLoss: 0.25596732\n",
      "Epoch: 500 \tLoss: 0.2511695\n",
      "Epoch: 600 \tLoss: 0.24788913\n",
      "Epoch: 700 \tLoss: 0.24576478\n",
      "Epoch: 800 \tLoss: 0.2445567\n",
      "Epoch: 900 \tLoss: 0.24291086\n",
      "Epoch: 1000 \tLoss: 0.24248724\n",
      "Epoch: 1100 \tLoss: 0.24198936\n",
      "Epoch: 1200 \tLoss: 0.24109025\n",
      "Epoch: 1300 \tLoss: 0.24087906\n",
      "Epoch: 1400 \tLoss: 0.24060279\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1500\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val = loss.eval({X: X_test, y: y_test})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test, y: y_test})\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we dont use the epoch number when generating batches, so we could just have a sinlge for loop rather than a double loop. Buts it importnant to think of training in terms of the number of epochs (roughly the number of times the algorithm wen through the training set).\n",
    "\n",
    "For each instance in the test set y_proba_val contains the estimated probabilit that it belongs to the positive class accoriding to the model. For example, here are the first 5 estimated probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00275856],\n",
       "       [0.9860338 ],\n",
       "       [0.98700815],\n",
       "       [0.7936355 ],\n",
       "       [0.94911295],\n",
       "       [0.02285618]], dtype=float32)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba_val[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify each instance, we can for for MLE and assign when greater than 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_proba_val >=0.5)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall you can adjust the threshold to try to improve accuracy, at the cost of lower recall and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8979591836734694"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_idx = y_pred.reshape(-1) # a 1D array rather than a column vector\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not too good, but reacall logistic regression has a linear decision boundary:\n",
    "\n",
    "Now this time, lets start over but this time adding all the bells and whistles:\n",
    "\n",
    "* Define the graph with a log_reg function\n",
    "* save checkpoints using a Saver at regular intervals during training, and save tge final model at the end of training\n",
    "* Restore the last checkpoint upon startup if training was interrupted\n",
    "* Define the graph using nice scopes so the graph looks good in TensorBoard\n",
    "* Add summaries to visiualize learning curves in TensotrBoard\n",
    "* Try teweaking some hpyer parameters\n",
    "\n",
    "Before we start we will4 more features to the inputs $x_1^{2}, x_2^{2}, x_3^{2}, x_4^{2}$ to show how adding features can imporve the model. \n",
    "\n",
    "Note you could use sklearn's polynomialfeatures, but we peforem this expansion manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enhanced = np.c_[X_train,np.square(X_train[:,1]),\n",
    "                        np.square(X_train[:,2]),\n",
    "                        X_train[:,1]**3,\n",
    "                        X_train[:,2]**3]\n",
    "\n",
    "X_test_enhanced = np.c_[X_test,np.square(X_test[:,1]),\n",
    "                       np.square(X_test[:,2]),\n",
    "                       X_test[:,1]**3,\n",
    "                        X_test[:,2]**3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 7)\n",
      "(200, 7)\n"
     ]
    }
   ],
   "source": [
    "#check the shapes\n",
    "print(X_train_enhanced.shape)\n",
    "print(X_test_enhanced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  1.97416561e+00, -1.09790942e-01,\n",
       "         3.89732987e+00,  1.20540509e-02,  7.69397462e+00,\n",
       "        -1.32342559e-03],\n",
       "       [ 1.00000000e+00,  2.05086911e+00,  9.08698847e-02,\n",
       "         4.20606412e+00,  8.25733594e-03,  8.62608698e+00,\n",
       "         7.50343165e-04],\n",
       "       [ 1.00000000e+00, -1.26188496e-01,  2.67732905e-01,\n",
       "         1.59235366e-02,  7.16809086e-02, -2.00936713e-03,\n",
       "         1.91913379e-02],\n",
       "       [ 1.00000000e+00,  7.64453701e-01,  6.54477348e-01,\n",
       "         5.84389461e-01,  4.28340599e-01,  4.46738687e-01,\n",
       "         2.80339220e-01],\n",
       "       [ 1.00000000e+00,  1.85118635e+00, -1.89645570e-01,\n",
       "         3.42689091e+00,  3.59654422e-02,  6.34381369e+00,\n",
       "        -6.82068678e-03]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_enhanced[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the logistic regression function. We will leave out the deifintion of the inputs x and the targets y. We could include them but, leavin them out will it make it easier to relplicate this functoin in a wide range of use cases. Perhaps we wil want to add some preprocesing steps for the inputs before feeind them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X,y,initializer = None, seed=42,learning_rate=0.01):\n",
    "    n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    with tf.name_scope(\"logistic_regression\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            if initializer is None:\n",
    "                initializer = tf.random_uniform([n_inputs_including_bias,1],-1.0,1.0,seed=seed)\n",
    "            #set thetas\n",
    "            theta = tf.Variable(initializer,name='theta')\n",
    "            logits = tf.matmul(X,theta,name='logits')\n",
    "            y_proba = tf.sigmoid(logits)\n",
    "        with tf.name_scope(\"train\"):\n",
    "            loss = tf.losses.log_loss(y,y_proba,scope='loss')\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            loss_summary = tf.summary.scalar(\"log_loss\",loss)\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return(y_proba,loss,training_op,loss_summary,init,saver)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more function to get the name of the log directory to save the summaries for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = 'tf_logs'\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return(\"{}/{}\".format(root_logdir,name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the graph and save the files using filewriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 2+4\n",
    "logdir = log_dir(\"logreg\")\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs +1),name=\"X\")\n",
    "y = tf.placeholder(tf.float32,shape=(None,1),name=\"y\")\n",
    "\n",
    "y_proba,loss,training_op,loss_summary,init,saver = logistic_regression(X,y)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually begin training our model. We will start by checking whether a previous training session was interrupted, and if so we will load the checkpoint and continue training from the epoch number we saved. In this example, we just save teh epoch number to a seperate file, but in chapter 11, we will see how to store the training step directly as part of the model. Using a non-trainable variable called global_step\n",
    "\n",
    "You can try interrupting training to verify that it doest ineed restore the last checkpoint when you start it again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.62877786\n",
      "Epoch: 500 \tLoss: 0.16100284\n",
      "Epoch: 1000 \tLoss: 0.11890707\n",
      "Epoch: 1500 \tLoss: 0.09718493\n",
      "Epoch: 2000 \tLoss: 0.083643876\n",
      "Epoch: 2500 \tLoss: 0.074311234\n",
      "Epoch: 3000 \tLoss: 0.06744907\n",
      "Epoch: 3500 \tLoss: 0.062173855\n",
      "Epoch: 4000 \tLoss: 0.057930898\n",
      "Epoch: 4500 \tLoss: 0.054515094\n",
      "Epoch: 5000 \tLoss: 0.051692456\n",
      "Epoch: 5500 \tLoss: 0.049304485\n",
      "Epoch: 6000 \tLoss: 0.047199287\n",
      "Epoch: 6500 \tLoss: 0.04536928\n",
      "Epoch: 7000 \tLoss: 0.043792583\n",
      "Epoch: 7500 \tLoss: 0.04236175\n",
      "Epoch: 8000 \tLoss: 0.041122686\n",
      "Epoch: 8500 \tLoss: 0.03999031\n",
      "Epoch: 9000 \tLoss: 0.038911253\n",
      "Epoch: 9500 \tLoss: 0.037978612\n",
      "Epoch: 10000 \tLoss: 0.037115537\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/my_logreg_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_logreg_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "\n",
    "    saver.save(sess, final_model_path)\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "    os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9797979797979798"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_proba_val >= 0.5)\n",
    "precision_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9797979797979798"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_idx = y_pred.reshape(-1) # a 1D array rather than a column vector\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check out the tensorboard afte completing this run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      " logdir: tf_logs/logreg-run-20191223190703\n",
      " batch_size: 36\n",
      " learning_rate: 0.004430375245218265\n",
      " training:.....................\n",
      "  precision: 0.9797979797979798\n",
      "  recall: 0.9797979797979798\n",
      "Iteration 1\n",
      " logdir: tf_logs/logreg-run-20191223190956\n",
      " batch_size: 75\n",
      " learning_rate: 0.0017826497151386947\n",
      " training:.....................\n",
      "  precision: 0.9696969696969697\n",
      "  recall: 0.9696969696969697\n",
      "Iteration 2\n",
      " logdir: tf_logs/logreg-run-20191223191123\n",
      " batch_size: 86\n",
      " learning_rate: 0.00203228544324115\n",
      " training:.....................\n",
      "  precision: 0.9696969696969697\n",
      "  recall: 0.9696969696969697\n",
      "Iteration 3\n",
      " logdir: tf_logs/logreg-run-20191223191235\n",
      " batch_size: 87\n",
      " learning_rate: 0.004491523825137997\n",
      " training:.....................\n",
      "  precision: 0.9797979797979798\n",
      "  recall: 0.9797979797979798\n",
      "Iteration 4\n",
      " logdir: tf_logs/logreg-run-20191223192847\n",
      " batch_size: 61\n",
      " learning_rate: 0.07963234721775589\n",
      " training:.....................\n",
      "  precision: 0.9801980198019802\n",
      "  recall: 1.0\n",
      "Iteration 5\n",
      " logdir: tf_logs/logreg-run-20191223193027\n",
      " batch_size: 92\n",
      " learning_rate: 0.0004634250583294876\n",
      " training:.....................\n",
      "  precision: 0.912621359223301\n",
      "  recall: 0.9494949494949495\n",
      "Iteration 6\n",
      " logdir: tf_logs/logreg-run-20191223193139\n",
      " batch_size: 74\n",
      " learning_rate: 0.047706818419354494\n",
      " training:.....................\n",
      "  precision: 0.98\n",
      "  recall: 0.98989898989899\n",
      "Iteration 7\n",
      " logdir: tf_logs/logreg-run-20191223193306\n",
      " batch_size: 58\n",
      " learning_rate: 0.0001694044709524274\n",
      " training:.....................\n",
      "  precision: 0.9\n",
      "  recall: 0.9090909090909091\n",
      "Iteration 8\n",
      " logdir: tf_logs/logreg-run-20191223193500\n",
      " batch_size: 61\n",
      " learning_rate: 0.04171461199412461\n",
      " training:.....................\n",
      "  precision: 0.9801980198019802\n",
      "  recall: 1.0\n",
      "Iteration 9\n",
      " logdir: tf_logs/logreg-run-20191223193647\n",
      " batch_size: 92\n",
      " learning_rate: 0.00010742922968438615\n",
      " training:.....................\n",
      "  precision: 0.8823529411764706\n",
      "  recall: 0.7575757575757576\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "\n",
    "n_search_iterations = 10\n",
    "\n",
    "for search_iteration in range(n_search_iterations):\n",
    "    batch_size = np.random.randint(1,100)\n",
    "    learning_rate = reciprocal(0.0001,0.1).rvs(random_state=search_iteration)\n",
    "    \n",
    "    n_inputs = 2+4\n",
    "    logdir = log_dir(\"logreg\")\n",
    "    \n",
    "    print(\"Iteration\",search_iteration)\n",
    "    print(\" logdir:\",logdir)\n",
    "    print(\" batch_size:\",batch_size)\n",
    "    print(\" learning_rate:\",learning_rate)\n",
    "    print(\" training:\",end =\"\")\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    X = tf.placeholder(tf.float32,shape=(None,n_inputs+1),name=\"X\")\n",
    "    y = tf.placeholder(tf.float32,shape=(None,1),name=\"y\")\n",
    "    \n",
    "    y_proba,loss,training_op,loss_summary,init,saver = logistic_regression(X,y,learning_rate=learning_rate)\n",
    "    \n",
    "    file_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    \n",
    "    n_epochs = 10001\n",
    "    n_batches = int(np.ceil(m/batch_size))\n",
    "    \n",
    "    final_model_path = \"./my_logreg_model_%d\" % search_iteration\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_index in range(n_batches):\n",
    "                X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "            file_writer.add_summary(summary_str, epoch)\n",
    "            if epoch % 500 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "\n",
    "        saver.save(sess, final_model_path)\n",
    "\n",
    "        print()\n",
    "        y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        y_pred = (y_proba_val >= 0.5)\n",
    "        \n",
    "        print(\"  precision:\", precision_score(y_test, y_pred))\n",
    "        print(\"  recall:\", recall_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TADA!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
